# KV Cache

## Use case
TBD

## Definition
A technique used in transformer models, particularly in large language models, to optimize inference speed and efficiency. It stores intermediate computations (specifically the "key" and "value" vectors from the attention mechanism) from previous processing steps, allowing the model to reuse them when processing subsequent tokens.

## Pros and Cons
### pros
* TBD 

### cons
* TBD


## Heuristics
* TBD

## Minimal Implementation
* PyTorch
TBD.

* TensorFlow
TBD.

## Reference
* TBD
